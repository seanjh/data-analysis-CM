{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data Source\n",
    "\n",
    "The dataset used for this assignment was provided by Julian McAuley and was found using the links suggested in the assginment (http://jmcauley.ucsd.edu/cse255/data/beer/Ratebeer.txt.gz). This data was collected as a part of the following studies (https://snap.stanford.edu/data/web-RateBeer.html):\n",
    "\n",
    "[1] J. McAuley, J. Leskovec, and D. Jurafsky. Learning attitudes and attributes from multi-aspect reviews. ICDM, 2012.\n",
    "\n",
    "[2] J. McAuley and J. Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. WWW, 2013.\n",
    "\n",
    "This dataset includes 2924164 user-provided reviews from the RateBeer website from the period of time 04/12/2000 to 01/13/2012. For the purposes of our analysis, we filtered the dataset down to a period of time +/- 1 year from the feature change date of 05/01/2009 (so 5/1/2008 - 5/1/2010) so that we could capture user behavior before and after the change. This narrows our dataset to be 801276 reviews from 9453 users. \n",
    "\n",
    "Note: Throughout our code, we assume that the file 'data/reduced_data.txt' has been generated. This can be generated using the ParseBeerData notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example Dataset Review Entry\n",
    "__ = {\n",
    "    'beer/ABV': 7.2,\n",
    "    'beer/beerId': '59261',\n",
    "    'beer/brewerId': '67',\n",
    "    'beer/name': 'Sierra Nevada Torpedo Extra IPA',\n",
    "    'beer/style': 'India Pale Ale &#40;IPA&#41;',\n",
    "    'review/appearance': 1.0,\n",
    "    'review/aroma': 0.8,\n",
    "    'review/overall': 0.9,\n",
    "    'review/palate': 0.8,\n",
    "    'review/taste': 1.0,\n",
    "    'review/text': 'Aroma is lemon and orange citrus, reddish orange hue, flavor is very well balanced between the malt and hop. this beer is very drinkable. It is not \"over the top\" like some of the double ipas. The alcohol content is well contained. The finish is hoppy as one would expect for this style. An excellent beer from the folks at Sierra Nevada!',\n",
    "    'review/timeStruct': {\n",
    "        'hour': 0,\n",
    "        'isdst': 0,\n",
    "        'mday': 17,\n",
    "        'min': 0,\n",
    "        'mon': 2,\n",
    "        'sec': 0,\n",
    "        'wday': 1,\n",
    "        'yday': 48,\n",
    "        'year': 2009\n",
    "    },\n",
    "   'review/timeUnix': 1234828800,\n",
    "   'user/profileName': 'Blazhock'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##User Level Results\n",
    "\n",
    "In addition to looking at site-level impact, we also hypothesized that the website changes may have altered the way that individual users engaged with and contributed to the overall community. This portion of the analysis looks at trends in select features and identifies how a user's contribution level before the remodel correlated with observed contribution changes after. \n",
    "\n",
    "We conclude this section by building a classification model to predict (based on a user's contributions before the \"split\") how the number of reviews contributed by the user change after the \"split\" [frome hereon referred to as \"pre-split\" and \"post-split\"].We test several different linear classifiers and evaluate how predictive the features identified really are. \n",
    "\n",
    "For the purposes of this analysis, we focused on the following user feature variables:\n",
    "* Number of Reviews Posted - Users more commited to the community will post more reviews\n",
    "* Review Diversity - Users who are more engaged will post more and more diverse information\n",
    "    * Types of Beers - Count\n",
    "    * Rating Scores - Number of Different, Average\n",
    "    * Review Length - Number of Different, Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import statements\n",
    "import unicodedata\n",
    "import random\n",
    "import calendar\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A variety of helper methods to group data structures accordingto \n",
    "\"\"\"\n",
    "def groupby_key(data, key_str):\n",
    "    key_map = {}\n",
    "    for datum in data:\n",
    "        key = datum.get(key_str)\n",
    "        key_map[key] = key_map.setdefault(key, [])\n",
    "        key_map[key].append(datum)\n",
    "    return key_map\n",
    "\n",
    "def groupby_key_count(data, key_str, func=None):\n",
    "    key_map = {}\n",
    "    for datum in data:\n",
    "        key = datum.get(key_str)\n",
    "        try:\n",
    "            key = func(key)\n",
    "        except: \n",
    "            key = key\n",
    "        key_map[key] = key_map.setdefault(key, 0) + 1\n",
    "    return key_map\n",
    "\n",
    "def group_time_split(user_array, key, func=None):\n",
    "    return [[groupby_key_count(time_per, key, func) for time_per in user] for user in user_array]\n",
    "\n",
    "\"\"\"\n",
    "Reservoir sampling given an iterable input and k for number of items to be sampled\n",
    "\"\"\"\n",
    "def reservoir_sample(iterator, k):\n",
    "    iterator = iter(iterator)\n",
    "    # fill the reservoir to start\n",
    "    result = [next(iterator) for _ in range(k)]\n",
    "    n = k\n",
    "    for item in iterator:\n",
    "        n += 1\n",
    "        s = random.randint(0, n)\n",
    "        if s < k:\n",
    "            result[s] = item\n",
    "    return result\n",
    "\"\"\"\n",
    "Calculates the average using dictionary keys as \"values\" and dictionary values as \"counts\"\n",
    "\"\"\"\n",
    "def avg_from_map(keymap):\n",
    "    count, total, a = 0, 0, None\n",
    "    for key in keymap:\n",
    "        if (key):\n",
    "            count += keymap[key]\n",
    "            total += key * keymap[key]\n",
    "    if (count):\n",
    "        a = total / count\n",
    "    return a\n",
    "\"\"\"\n",
    "average from user tuple array\n",
    "\"\"\"\n",
    "def avg_from_user_array_tuple(array):\n",
    "    out = []\n",
    "    for user in array:\n",
    "        user_avg = []\n",
    "        for time in user:\n",
    "            count, total, a = 0, 0, None\n",
    "            for item in time:\n",
    "                count += item[1]\n",
    "                total += item[0] * item[1]\n",
    "            if count > 0: \n",
    "                a = total/count\n",
    "            user_avg.append(a) \n",
    "        out.append(user_avg)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse and read in dataset from reduced_data.txt (as produced by our parsing code)\n",
    "parsed_red = parse_json('data/reduced_data.txt', normalize=True)\n",
    "dset = [i for i in parsed_red]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse dataset, grouping reviews by username\n",
    "users = groupby_key(dset,'user/profileName')\n",
    "user_vals = users.values()\n",
    "print 'number of users', len(user_vals)\n",
    "\n",
    "# split each users' group of reviews into two buckets: before and after site changes\n",
    "user_vals_split = []\n",
    "split_timestamp = calendar.timegm(datetime(2009,5,1).timetuple())\n",
    "\n",
    "for i , posts in enumerate(user_vals):\n",
    "    pre = [post for post in posts if post.get('review/timeUnix') < split_timestamp]\n",
    "    post = [post for post in posts if post.get('review/timeUnix') >= split_timestamp]\n",
    "#     Only look at users who already contributed in the prior period\n",
    "    if len(pre) > 0:\n",
    "        user_vals_split.append([pre, post])\n",
    "\n",
    "# sort reviews according to the number of reviews users contributed before \n",
    "user_vals_split = sorted(user_vals_split, key=lambda user: len(user[0]))\n",
    "print 'number of users post split', len(user_vals_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###High-level Feature Trends\n",
    "As we brainstormed features which may reflect a user's engagement level, we eventually settled on the following variables: number of reviews contributed, review diversity (types of beers being reviewed), average review rating (score), and average review length (word count). \n",
    "\n",
    "Specifically, we were interested in seeing how each of these variables changed \"pre-split\" to \"post-split\" based on the user's relative amount of reviews contributed \"pre-split\". \n",
    "\n",
    "In order to look at these features, we sorted the user dataset based on the number of reviews each user contributed \"pre-split\" and calculated a plus-minus value for each of the variables we looked at (plus-minus calculated as value_post_split - value_pre_split). We then plotted these values for the entire population and calculated mean, median, and standard deviation.\n",
    "\n",
    "Over the course of our analysis, we noticed that high-usage users typically exhibited more extreme changes in plus-minus. For this reason, we also performed the mean/median/stdev analysis for the top 5% of users. In order to provide a control for these high-usage users, we paired the high-usage statistics with the numbers for a ramdomly sampled group of the same size from the entire dataset as well as the \"long tail\" of 95% other users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample \"top\" 5% and \"low\" 95% of users\n",
    "users_top = user_vals_split[int(math.floor(len(user_vals_split)*0.95)):]\n",
    "users_low = user_vals_split[:int(math.floor(len(user_vals_split)*0.95))]\n",
    "\n",
    "# sample \"random\" user group to be equal to the \"top\" group\n",
    "sample_size = len(users_top)\n",
    "users_sampled = reservoir_sample(user_vals_split, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot utility to plot the users's plus minus\n",
    "Prints out the mean, median, and standard deviation for the plus minus \n",
    "\"\"\"\n",
    "def plot_diff(user_list, func=lambda x: x, plot=True):\n",
    "    user_pre = [func(user[0]) for user in user_list]\n",
    "    user_post = [func(user[1]) for user in user_list]\n",
    "\n",
    "    np_pre = np.array(user_pre)\n",
    "    np_post = np.array(user_post)\n",
    "    np_pm = np_post - np_pre\n",
    "    print \"Mean p/m: \", np.mean(np_pm)\n",
    "    print \"Median p/m: \", np.median(np_pm)\n",
    "    print \"Std Dev p/m: \", np.std(np_pm)\n",
    "\n",
    "    ind = np.arange(len(np_pm))\n",
    "    if (plot):\n",
    "        fig, ax = plt.subplots()\n",
    "        scatt = ax.scatter(ind, np_pm, c=\"grey\",s=10,edgecolor='none')\n",
    "        mean = ax.axhline(y=np.mean(np_pm),xmin=0,xmax=ind[len(ind) - 1],c=\"blue\",linewidth=0.9, linestyle = '-', zorder=1, label='mean')\n",
    "        median = ax.axhline(y=np.median(np_pm),xmin=0,xmax=ind[len(ind) - 1],c=\"red\",linewidth=0.9,linestyle = '--', zorder=2, label='median')\n",
    "        plt.legend(loc=3, ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Counts\n",
    "Based on the results below, we notice that, across the board, user contribution (as measured by review count) seem to have decreased after the profile-page change (counter what we would have predicted based on the increased behavioral incentive on the profile page). This includes users who didn't contribute in the second period at all (reflecting a lack of user retention). \n",
    "\n",
    "We notice that, on average, high-usage users seem to have skewed the overall average review count downwards as the average \"high-usage\" user had a review count decrease of 76 as compared to ~3 for both the long-tail and the random sample group (which makes sense considering a finite cap in number of beers to be reviewed). We do notice that there does appear to be a fair number of \"mid-usage\" users who did have the number of reviews contributed increase after the change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# review counts\n",
    "# all users stats\n",
    "print 'all user review counts'\n",
    "plot_diff(user_vals_split, lambda x: len(x))\n",
    "plt.xlabel('User ID (sorted by pre-split contributions)')\n",
    "plt.ylabel('review after - reviews before')\n",
    "plt.title('Review Count Plus-Minus')\n",
    "print\n",
    "\n",
    "# top users stats\n",
    "print 'top user review counts', len(users_top)\n",
    "plot_diff(users_top, lambda x: len(x), False)\n",
    "print\n",
    "\n",
    "# low users stats\n",
    "print 'low user review counts', len(users_low)\n",
    "plot_diff(users_low, lambda x: len(x), False)\n",
    "print\n",
    "\n",
    "# sampled users stats\n",
    "print 'sampled user review counts', len(users_sampled)\n",
    "plot_diff(users_sampled, lambda x: len(x), False)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove users who did not post reviews after the split (for rest of metrics)\n",
    "users_split_active = [user for user in user_vals_split if len(user[1]) > 0]\n",
    "users_top_active = users_split_active[int(math.floor(len(users_split_active)*0.95)):]\n",
    "users_low_active = users_split_active[:int(math.floor(len(users_split_active)*0.95))]\n",
    "users_sampled_active = reservoir_sample(users_split_active, len(users_top_active))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Number of Beer Styles Reviewed\n",
    "Based on the results below, we notice that, across the board, the average number of beer styles reviewed decreased after the profile-page change (consistent with the way that the new page encourages users to specialize). However, part of this change may have to do with the overall decrease in number of reviews, so it doesn't appear conclusive. We notice that, on average, high-usage users have a steeper drop in style count than the whole and random sample. \n",
    "\n",
    "Looking at the plot, we notice that there does appear to be a distinct pattern of having \"low-contribution\" users be more likely to have a positive plus-minus in this variable than \"high-contribution\" users. This makes sense as \"low-usage\" users have more beers available to them to review while \"high-usage\" users have less or the fact that \"high-usage\" users are more aware of their profile page. This seems to support the inconclusive nature of this feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# average number of styles reviewed before/after change\n",
    "print 'all users style counts'\n",
    "styles_all = group_time_split(users_split_active, 'beer/style')\n",
    "# want number of styles\n",
    "styles_all = map(lambda x: [len(x[0].keys()), len(x[1].keys())], styles_all)\n",
    "plot_diff(styles_all)\n",
    "plt.xlabel('User ID (sorted by pre-split contributions)')\n",
    "plt.ylabel('avg beer styles after - avg beer styles before')\n",
    "plt.title('Average Beer Style Count Plus-Minus')\n",
    "print\n",
    "\n",
    "print 'top users style counts'\n",
    "styles_top = group_time_split(users_top_active, 'beer/style')\n",
    "# want number of styles\n",
    "styles_top = map(lambda x: [len(x[0].keys()), len(x[1].keys())], styles_top)\n",
    "plot_diff(styles_top, lambda x: x, False)\n",
    "print\n",
    "\n",
    "print 'low users style counts'\n",
    "styles_low = group_time_split(users_low_active, 'beer/style')\n",
    "# want number of styles\n",
    "styles_low = map(lambda x: [len(x[0].keys()), len(x[1].keys())], styles_low)\n",
    "plot_diff(styles_low, lambda x: x, False)\n",
    "print\n",
    "\n",
    "print 'sample users style counts'\n",
    "styles_samp = group_time_split(users_sampled_active, 'beer/style')\n",
    "# want number of styles\n",
    "styles_samp = map(lambda x: [len(x[0].keys()), len(x[1].keys())], styles_samp)\n",
    "plot_diff(styles_samp, lambda x: x, False)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Overall Rating\n",
    "Based on the results below, we notice that, across the board, the average overall rating very slightly increased after the profile-page change. However, the increase is very small (smaller than 1%), and seems to reflect that the profile page change didn't markedly impact ratings. \n",
    "\n",
    "We note that the standard deviation for high-usage users is significantly smaller for this variable (perhaps due to longtime users have an anchoring affect to what ratings \"should\" be or having less reviews in the second period)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# average overall ratings before/after change\n",
    "print 'all users rating avg'\n",
    "rates_all = group_time_split(users_split_active, 'review/overall', lambda x: float(x))\n",
    "rates_all = map(lambda x: [[(rating,  x[0][rating]) for rating in x[0].keys()], [(rating,  x[1][rating]) for rating in x[1].keys()]], rates_all)\n",
    "avg_rates = avg_from_user_array_tuple(rates_all)\n",
    "plot_diff(avg_rates)\n",
    "plt.xlabel('User ID (sorted by pre-split contributions)')\n",
    "plt.ylabel('avg overall rating after - avg overall rating before')\n",
    "plt.title('Average Overall Rating Plus-Minus (Max Rating = 1)')\n",
    "print\n",
    "\n",
    "print 'top users rating avg'\n",
    "rates_top = group_time_split(users_top_active, 'review/overall', lambda x: float(x))\n",
    "rates_top = map(lambda x: [[(rating,  x[0][rating]) for rating in x[0].keys()], [(rating,  x[1][rating]) for rating in x[1].keys()]], rates_top)\n",
    "avg_rates = avg_from_user_array_tuple(rates_top)\n",
    "plot_diff(avg_rates, lambda x: x, False)\n",
    "print\n",
    "\n",
    "print 'low users rating avg'\n",
    "rates_low = group_time_split(users_low_active, 'review/overall', lambda x: float(x))\n",
    "rates_low = map(lambda x: [[(rating,  x[0][rating]) for rating in x[0].keys()], [(rating,  x[1][rating]) for rating in x[1].keys()]], rates_low)\n",
    "avg_rates = avg_from_user_array_tuple(rates_low)\n",
    "plot_diff(avg_rates, lambda x: x, False)\n",
    "print\n",
    "\n",
    "print 'sampled users rating avg'\n",
    "rates_samp = group_time_split(users_sampled_active, 'review/overall', lambda x: float(x))\n",
    "rates_samp = map(lambda x: [[(rating,  x[0][rating]) for rating in x[0].keys()], [(rating,  x[1][rating]) for rating in x[1].keys()]], rates_samp)\n",
    "avg_rates = avg_from_user_array_tuple(rates_samp)\n",
    "styles_plot = plot_diff(avg_rates, lambda x: x, False)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Review Length\n",
    "Based on the results below, we notice that, across the board, the average review length very slightly increased after the profile-page change. However, the increase is very small (smaller than 1 word), and seems to reflect that the profile page change didn't markedly impact review length. \n",
    "\n",
    "We note that the standard deviation for high-usage users is significantly smaller for this variable (perhaps due to longtime users have an anchoring affect to what reviews \"should\" be or having less reviews in the second period)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# average review lengths before/after change\n",
    "print 'all users review length avg'\n",
    "reviews_all = group_time_split(users_split_active, 'review/text', lambda x: float(x))\n",
    "reviews_all = map(lambda x: [[(len(review.split()),  x[0][review]) for review in x[0].keys()], [(len(review.split()),  x[1][review]) for review in x[1].keys()]], reviews_all)\n",
    "avg_review_len = avg_from_user_array_tuple(reviews_all)\n",
    "styles_plot = plot_diff(avg_review_len)\n",
    "plt.xlabel('User ID (sorted by pre-split contributions)')\n",
    "plt.ylabel('avg review length after - avg review length before')\n",
    "plt.title('Average Review Length Plus-Minus')\n",
    "print\n",
    "\n",
    "print 'top users review length avg'\n",
    "reviews_top = group_time_split(users_top_active, 'review/text', lambda x: float(x))\n",
    "reviews_top = map(lambda x: [[(len(review.split()),  x[0][review]) for review in x[0].keys()], [(len(review.split()),  x[1][review]) for review in x[1].keys()]], reviews_top)\n",
    "avg_review_len = avg_from_user_array_tuple(reviews_top)\n",
    "styles_plot = plot_diff(avg_review_len, lambda x: x, False)\n",
    "print\n",
    "\n",
    "print 'low users review length avg'\n",
    "reviews_low = group_time_split(users_low_active, 'review/text', lambda x: float(x))\n",
    "reviews_low = map(lambda x: [[(len(review.split()),  x[0][review]) for review in x[0].keys()], [(len(review.split()),  x[1][review]) for review in x[1].keys()]], reviews_low)\n",
    "avg_review_len = avg_from_user_array_tuple(reviews_low)\n",
    "styles_plot = plot_diff(avg_review_len, lambda x: x, False)\n",
    "print\n",
    "\n",
    "print 'sampled users review length avg'\n",
    "reviews_samp = group_time_split(users_sampled_active, 'review/text', lambda x: float(x))\n",
    "reviews_samp = map(lambda x: [[(len(review.split()),  x[0][review]) for review in x[0].keys()], [(len(review.split()),  x[1][review]) for review in x[1].keys()]], reviews_samp)\n",
    "avg_review_len = avg_from_user_array_tuple(reviews_samp)\n",
    "styles_plot = plot_diff(avg_review_len, lambda x: x, False)\n",
    "print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Explore Potential  Correlation\n",
    "Based on the earlier high-level analysis performed, it seemed apparent that some of the features visibly exhibited patterns suggesting some form of correlation. In order to further explore this idea, we performed a correlation analysis (focusing on linear correlation) on each \"pre-split\" feature -- total number of reviews, number of styles reviewed, average overall rating, number of distinct ratings, average review length, and number of distinct review lengths. \n",
    "\n",
    "For each of these features, we calculated the pearson correlation coefficient against both the number of post-split reviews and the review count plus-minus for each user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Setting up Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build feature vectors\n",
    "x, y_after, y_diff = [], [], []\n",
    "for user in user_vals_split:\n",
    "    pre_char = user[0]\n",
    "    style_map = groupby_key_count(pre_char, 'beer/style')\n",
    "    rating_map = groupby_key_count(pre_char, 'review/overall')\n",
    "    review_map = groupby_key_count(pre_char, 'review/text', lambda x : len(x.split()))\n",
    "    total_count = len(pre_char)\n",
    "    style_count = len(style_map.keys())\n",
    "    avg_rating = avg_from_map(rating_map)\n",
    "    rating_count = len(rating_map.keys())\n",
    "    avg_review = avg_from_map(review_map)\n",
    "    review_count = len(review_map.keys())\n",
    "    # throw away points that don't have rating / review\n",
    "    if (avg_rating is not None and avg_review is not None):\n",
    "        x.append([total_count, style_count, avg_rating, rating_count, avg_review, review_count])\n",
    "        y_after.append(len(user[1]))\n",
    "        y_diff.append(len(user[1])-len(user[0]))\n",
    "data_pool = zip(x,y_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def balance_pool(pool):\n",
    "    \"\"\"\n",
    "    Given a pool of year-key formatted unbalanced outcome data, return a balanced set where positive outcomes have equal number of occurances to negative outcomes\n",
    "    The larger set is randomly sampled to reduce its size by using reservoir_sample\n",
    "    \"\"\"\n",
    "    newpool = []\n",
    "    neg = []\n",
    "    pos = []\n",
    "    for user in pool:\n",
    "        if user[-1] < 0:\n",
    "            neg.append(user)\n",
    "        else:\n",
    "            pos.append(user)\n",
    "    minlen = min(len(pos), len(neg))\n",
    "    for elem in reservoir_sample(neg, minlen):\n",
    "        newpool.append(elem)\n",
    "    for elem in reservoir_sample(pos, minlen):\n",
    "        newpool.append(elem)\n",
    "    return newpool\n",
    "\n",
    "def create_train_test(pool, test, train):\n",
    "    \"\"\"\n",
    "    Split the data pool created in create_data_pool randomly into a 80/20 split between training data and testing data\n",
    "    Shuffles all the years and randomly splits 80/20 between training and test\n",
    "    Should only be ran once to randomly split train/test data as it will return different results between runs\n",
    "    \"\"\"\n",
    "    random.shuffle(pool)\n",
    "    ind = int(len(pool) * 0.8)\n",
    "    train += pool[:ind]\n",
    "    test += pool[ind:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Pearson Correlation Coefficient\n",
    "Based on our results for the Correlation Coefficient, only one of our features exibits a value suggesting a good linear fit: the correlation coefficient between number of pre-split reviews and the number of post-split reviews (a correlation coefficient of ~0.84). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate pearson correlation coefficient for each variable\n",
    "# use to predict plus-minus and actual number of reviews after \n",
    "import scipy.stats\n",
    "np_x = np.array(x)\n",
    "np_y_after = np.array(y_after)\n",
    "np_y_diff = np.array(y_diff)\n",
    "\"\"\"\n",
    "Index to Pre-Split Feature Mapping\n",
    "0 - total number of reviews\n",
    "1 - number of styles reviewed\n",
    "2 - average overall rating\n",
    "3 - number of distinct ratings\n",
    "4 - average review length\n",
    "5 - number of distinct review lengths\n",
    "\"\"\"\n",
    "print \"Pearson Correlation Coefficients Against Post-Split Number of Reviews\"\n",
    "for i in xrange(len(np_x[0])):\n",
    "    print \"pearson for char\", i, \":\", scipy.stats.pearsonr(np_x[:,i], np_y_after)\n",
    "print\n",
    "print \"Pearson Correlation Coefficients Against Number of Reviews Plus-Minus\"\n",
    "for i in xrange(len(np_x[0])):\n",
    "    print \"Difference -- pearson for char\", i, \":\", scipy.stats.pearsonr(np_x[:,i], np_y_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize this relationship, we plotted these two features and fit a linear line against them. The result of this operation is shown below. \n",
    "\n",
    "The linear fit has a R^2 value of ~0.71, which is not great. The few high-contribution users in the top-right corner seem to be driving the positive linear relationship and the high concentration of user points in the lower-left hand corner seem to suggest that a linear model might not be the right fit for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err= scipy.stats.linregress(np_x[:,0], np_y_after)\n",
    "print \"r-squared:\", r_value**2\n",
    "\n",
    "fit = np.polyfit(np_x[:,0],np_y_after,1)\n",
    "fit_fn = np.poly1d(fit)\n",
    "_ = plt.plot(np_x[:,0], np_y_after, 'b.', x, fit_fn(x), '--k')\n",
    "plt.ylim(0)\n",
    "plt.xlabel('# of Pre-Split Reviews')\n",
    "plt.ylabel('# of Post-Split Reviews')\n",
    "plt.title('Linear Regression of Pre/Post Split Review Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While performing the linear correlation analysis, we noticed several interesting visually-identifiable, non-linear trends when plotting the following variables against the # of post-split review count:\n",
    "\n",
    "* Number of Pre-Split Styles Reviewed\n",
    "* Average Pre-Split Rating\n",
    "* Average Review Length\n",
    "\n",
    "Specifically we see the following:\n",
    "\n",
    "* There does seem to be a positive correlation between reviewed number of styles (before) and number of reviews (after)\n",
    "* Most users in our dataset give ratings around 0.7 (dense) - there is also a peak number of post-split reviews at this average\n",
    "* Most users in our dataset wrote reviews around 50 words in length (dense) - there is also a peak number of post-split reviews at this average\n",
    "* Users who wrote extremely long/short/positive/negative reviews have few post-split reviews (both graphs taper at the ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(np_x[:,1], np_y_after, 'b.')\n",
    "plt.ylim(0)\n",
    "plt.xlabel('# of Pre-Split Styles Reviewed')\n",
    "plt.ylabel('# of Post-Split Reviews')\n",
    "plt.title('Correlation btwn Pre Styles Types and Post Review Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_= plt.plot(np_x[:,2], np_y_after, 'b.')\n",
    "plt.ylim(0)\n",
    "plt.xlabel('Average Pre-Split Overall Rating of Reviews')\n",
    "plt.ylabel('# of Post-Split Reviews')\n",
    "plt.title('Correlation btwn Pre Average Rating and Post Review Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_= plt.plot(np_x[:,4], np_y_after, 'b.')\n",
    "plt.ylim(0)\n",
    "plt.xlabel('Average Pre-Split Review Length (Word Count)')\n",
    "plt.ylabel('# of Post-Split Reviews')\n",
    "plt.title('Correlation btwn Average Review Length and Post Review Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Explore Binary Classifier\n",
    "Even though no clear linear correlation is apparent to us based on our analysis so far of the select features, we thought it'd be worthwhile to test different linear classification methods with our selected feature (especially considering the potential non-linear correlation we've noticed). By doing so, we would be able to evaluate how predictive our set of features really is (based on an actual model evaluation accuracy score). \n",
    "\n",
    "Specifically, we were interested in whether our identified list of features could be used to predict whether a user's review contribution would increase or decrease followiing the change in the website. As we were attempting to create a positive/negative classifier, we first created a balanced pool of data to work from -- where there were an equal number of positive and negative samples (where positive/negative is determined by the user's review count plus-minus).  Following this, we split the data pool 80/20 into a training and test set. We chose to perform cross_validation (n=4) to evaluate different models and used a standard mean/standard deviation scaling to normalize our feature values. \n",
    "\n",
    "For the purposes of this assignment, we looked at 4 different sklearn classifiers: rbf SVM, linear SVM, dual-l2 logistic regression, non-dual-l2 logistic regression. From a cross-validation score, the model which performed best was the rbf SVM algorithm. \n",
    "\n",
    "Overall, this resulted in a 62% accuracy (consistent with the output from the other classifiers) -- a score that is not very good and not that much better than random. This leads us to believe that the features we selected are not strongly predictive of future review contributions to the RateBeer website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bal_data_pool = balance_pool(data_pool)\n",
    "test, train = [], []\n",
    "create_train_test(bal_data_pool, test, train)\n",
    "\n",
    "train = [[user[0] for user in train], [1 if user[1] > 0 else -1 for user in train]]\n",
    "test = [[user[0] for user in test], [1 if user[1] > 0 else -1 for user in test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm, neighbors, qda, metrics, cross_validation\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([('Scaler', StandardScaler()),\n",
    "#                 ('Log-Reg', linear_model.LogisticRegression(penalty='l2', dual=True))])\n",
    "#                     ('Log-Reg', linear_model.LogisticRegression(penalty='l2', dual=False))])\n",
    "#                     ('SVC-linear', svm.SVC(kernel='linear'))])\n",
    "                    ('SVC-rbf', svm.SVC(kernel='rbf'))])\n",
    "\n",
    "cv = cross_validation.KFold(len(train[0]), n_folds=4, shuffle=True)\n",
    "scores = cross_validation.cross_val_score(clf, train[0], train[1], cv=cv)\n",
    "print \"Cross Validation Scores:\", scores\n",
    "print \"Average Cross Validation Score:\", np.average(scores)\n",
    "print\n",
    "# peforms test on selected model\n",
    "clf = clf.fit(train[0], train[1])\n",
    "predicted = clf.predict(test[0])\n",
    "print \"Model Accuracy:\", metrics.accuracy_score(test[1], predicted)\n",
    "print \"Confusion Matrix\"\n",
    "print metrics.confusion_matrix(test[1], predicted)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
